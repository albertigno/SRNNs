{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blind-ancient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: albertigno\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from MyDataset import *\n",
    "import torch, time, os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.gridspec import GridSpec\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Running on: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "primary-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.3\n",
    "batch_size = 256 # default 256\n",
    "learning_rate = 1e-4 # default 1e-4\n",
    "time_window = 50 # shd 50, nmnist 25-30\n",
    "dataset_path = r'./../../datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "given-reviewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test set...\n",
      "num sample: 2264\n",
      "torch.Size([2264, 50, 700]) torch.Size([2264, 20])\n",
      "loading training set...\n",
      "num sample: 8156\n",
      "torch.Size([8156, 50, 700]) torch.Size([8156, 20])\n",
      "loading data with pytorch\n"
     ]
    }
   ],
   "source": [
    "train_path = dataset_path+'/shd_digits/shd_train.h5'\n",
    "test_path = dataset_path+'/shd_digits/shd_test.h5'\n",
    "# load datasets\n",
    "print(\"loading test set...\")\n",
    "test_dataset = MyDataset(test_path, 'hd_digits', time_window, device)\n",
    "print(\"loading training set...\")\n",
    "train_dataset = MyDataset(train_path, 'hd_digits', time_window, device)\n",
    "print(\"loading data with pytorch\")\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nasty-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snn_models import *\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport snn_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "improving-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "num_epochs = 2\n",
    "import numpy as np\n",
    "\n",
    "tau_m = [0.8305, 'adp']\n",
    "num_hidden = [128, 256, 512, 1024, 2048]\n",
    "delay_mode = ['nodelay','delay']\n",
    "vreset = [0.0, 0.1]\n",
    "\n",
    "num = len(tau_m)*len(num_hidden)*len(delay_mode)*len(vreset)\n",
    "\n",
    "x, y, z, w = np.meshgrid(tau_m, num_hidden, delay_mode, vreset)\n",
    "x=x.reshape(num)\n",
    "y=y.reshape(num)\n",
    "z=z.reshape(num)\n",
    "w=w.reshape(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "generic-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------TRAINING shd_rnn_128_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51802\n",
      "Step [20/31], Loss: 0.48602\n",
      "Step [30/31], Loss: 0.48140\n",
      "Time elasped: 2.271711826324463\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47575\n",
      "Step [20/31], Loss: 0.47101\n",
      "Step [30/31], Loss: 0.46618\n",
      "Time elasped: 2.341134548187256\n",
      "-------TRAINING shd_rnn_128_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51764\n",
      "Step [20/31], Loss: 0.48537\n",
      "Step [30/31], Loss: 0.47926\n",
      "Time elasped: 2.2992775440216064\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47316\n",
      "Step [20/31], Loss: 0.46874\n",
      "Step [30/31], Loss: 0.46297\n",
      "Time elasped: 2.252021074295044\n",
      "-------TRAINING shd_rnn_128_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.57441\n",
      "Step [20/31], Loss: 0.48941\n",
      "Step [30/31], Loss: 0.48364\n",
      "Time elasped: 2.655921220779419\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47724\n",
      "Step [20/31], Loss: 0.46702\n",
      "Step [30/31], Loss: 0.46268\n",
      "Time elasped: 2.716430902481079\n",
      "-------TRAINING shd_rnn_128_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51849\n",
      "Step [20/31], Loss: 0.48303\n",
      "Step [30/31], Loss: 0.47406\n",
      "Time elasped: 2.657303810119629\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46232\n",
      "Step [20/31], Loss: 0.45095\n",
      "Step [30/31], Loss: 0.44771\n",
      "Time elasped: 2.6518940925598145\n",
      "-------TRAINING shd_rnn_128_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49091\n",
      "Step [20/31], Loss: 0.48372\n",
      "Step [30/31], Loss: 0.47597\n",
      "Time elasped: 2.4533579349517822\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46930\n",
      "Step [20/31], Loss: 0.46345\n",
      "Step [30/31], Loss: 0.45734\n",
      "Time elasped: 2.4500656127929688\n",
      "-------TRAINING shd_rnn_128_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49791\n",
      "Step [20/31], Loss: 0.48427\n",
      "Step [30/31], Loss: 0.48111\n",
      "Time elasped: 2.42817759513855\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47286\n",
      "Step [20/31], Loss: 0.46700\n",
      "Step [30/31], Loss: 0.46194\n",
      "Time elasped: 2.435910940170288\n",
      "-------TRAINING shd_rnn_128_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.55114\n",
      "Step [20/31], Loss: 0.48650\n",
      "Step [30/31], Loss: 0.48098\n",
      "Time elasped: 2.8012986183166504\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47215\n",
      "Step [20/31], Loss: 0.46479\n",
      "Step [30/31], Loss: 0.45860\n",
      "Time elasped: 2.816077709197998\n",
      "-------TRAINING shd_rnn_128_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52322\n",
      "Step [20/31], Loss: 0.48272\n",
      "Step [30/31], Loss: 0.47335\n",
      "Time elasped: 2.8256936073303223\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46272\n",
      "Step [20/31], Loss: 0.45451\n",
      "Step [30/31], Loss: 0.44771\n",
      "Time elasped: 2.769707202911377\n",
      "-------TRAINING shd_rnn_256_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.54405\n",
      "Step [20/31], Loss: 0.48422\n",
      "Step [30/31], Loss: 0.47910\n",
      "Time elasped: 2.262354850769043\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47192\n",
      "Step [20/31], Loss: 0.46808\n",
      "Step [30/31], Loss: 0.46212\n",
      "Time elasped: 2.2575199604034424\n",
      "-------TRAINING shd_rnn_256_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52026\n",
      "Step [20/31], Loss: 0.48999\n",
      "Step [30/31], Loss: 0.48366\n",
      "Time elasped: 2.245162010192871\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47315\n",
      "Step [20/31], Loss: 0.46984\n",
      "Step [30/31], Loss: 0.46176\n",
      "Time elasped: 2.3098230361938477\n",
      "-------TRAINING shd_rnn_256_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.48643\n",
      "Step [20/31], Loss: 0.46940\n",
      "Step [30/31], Loss: 0.45612\n",
      "Time elasped: 2.737924575805664\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43904\n",
      "Step [20/31], Loss: 0.42958\n",
      "Step [30/31], Loss: 0.41994\n",
      "Time elasped: 2.708906412124634\n",
      "-------TRAINING shd_rnn_256_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51667\n",
      "Step [20/31], Loss: 0.47759\n",
      "Step [30/31], Loss: 0.46898\n",
      "Time elasped: 2.7452778816223145\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45528\n",
      "Step [20/31], Loss: 0.44328\n",
      "Step [30/31], Loss: 0.43390\n",
      "Time elasped: 2.7334420680999756\n",
      "-------TRAINING shd_rnn_256_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51692\n",
      "Step [20/31], Loss: 0.48626\n",
      "Step [30/31], Loss: 0.47981\n",
      "Time elasped: 2.437278985977173\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.47207\n",
      "Step [20/31], Loss: 0.46721\n",
      "Step [30/31], Loss: 0.46075\n",
      "Time elasped: 2.4776790142059326\n",
      "-------TRAINING shd_rnn_256_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49283\n",
      "Step [20/31], Loss: 0.48314\n",
      "Step [30/31], Loss: 0.47562\n",
      "Time elasped: 2.45568585395813\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46339\n",
      "Step [20/31], Loss: 0.45475\n",
      "Step [30/31], Loss: 0.44812\n",
      "Time elasped: 2.4508373737335205\n",
      "-------TRAINING shd_rnn_256_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51236\n",
      "Step [20/31], Loss: 0.47957\n",
      "Step [30/31], Loss: 0.46516\n",
      "Time elasped: 2.783165693283081\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45139\n",
      "Step [20/31], Loss: 0.44147\n",
      "Step [30/31], Loss: 0.42861\n",
      "Time elasped: 2.80129337310791\n",
      "-------TRAINING shd_rnn_256_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49995\n",
      "Step [20/31], Loss: 0.48035\n",
      "Step [30/31], Loss: 0.46176\n",
      "Time elasped: 2.8248655796051025\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44779\n",
      "Step [20/31], Loss: 0.43944\n",
      "Step [30/31], Loss: 0.42788\n",
      "Time elasped: 2.811703681945801\n",
      "-------TRAINING shd_rnn_512_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50604\n",
      "Step [20/31], Loss: 0.48154\n",
      "Step [30/31], Loss: 0.47383\n",
      "Time elasped: 2.3408000469207764\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46399\n",
      "Step [20/31], Loss: 0.45438\n",
      "Step [30/31], Loss: 0.44894\n",
      "Time elasped: 2.3242850303649902\n",
      "-------TRAINING shd_rnn_512_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49690\n",
      "Step [20/31], Loss: 0.47634\n",
      "Step [30/31], Loss: 0.46339\n",
      "Time elasped: 2.3498733043670654\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44913\n",
      "Step [20/31], Loss: 0.44537\n",
      "Step [30/31], Loss: 0.43452\n",
      "Time elasped: 2.3382327556610107\n",
      "-------TRAINING shd_rnn_512_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51644\n",
      "Step [20/31], Loss: 0.47202\n",
      "Step [30/31], Loss: 0.45860\n",
      "Time elasped: 2.8329362869262695\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44053\n",
      "Step [20/31], Loss: 0.43265\n",
      "Step [30/31], Loss: 0.42024\n",
      "Time elasped: 2.849351406097412\n",
      "-------TRAINING shd_rnn_512_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51113\n",
      "Step [20/31], Loss: 0.47093\n",
      "Step [30/31], Loss: 0.45463\n",
      "Time elasped: 2.891587972640991\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43665\n",
      "Step [20/31], Loss: 0.42768\n",
      "Step [30/31], Loss: 0.42103\n",
      "Time elasped: 2.8514134883880615\n",
      "-------TRAINING shd_rnn_512_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49272\n",
      "Step [20/31], Loss: 0.47506\n",
      "Step [30/31], Loss: 0.46395\n",
      "Time elasped: 2.4885470867156982\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45136\n",
      "Step [20/31], Loss: 0.44146\n",
      "Step [30/31], Loss: 0.43488\n",
      "Time elasped: 2.4463038444519043\n",
      "-------TRAINING shd_rnn_512_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49551\n",
      "Step [20/31], Loss: 0.47662\n",
      "Step [30/31], Loss: 0.46533\n",
      "Time elasped: 2.4763059616088867\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45472\n",
      "Step [20/31], Loss: 0.44482\n",
      "Step [30/31], Loss: 0.44251\n",
      "Time elasped: 2.4797494411468506\n",
      "-------TRAINING shd_rnn_512_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51458\n",
      "Step [20/31], Loss: 0.47883\n",
      "Step [30/31], Loss: 0.46468\n",
      "Time elasped: 2.8221981525421143\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45184\n",
      "Step [20/31], Loss: 0.44212\n",
      "Step [30/31], Loss: 0.43168\n",
      "Time elasped: 2.7900376319885254\n",
      "-------TRAINING shd_rnn_512_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.48725\n",
      "Step [20/31], Loss: 0.46160\n",
      "Step [30/31], Loss: 0.44302\n",
      "Time elasped: 2.815938711166382\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43066\n",
      "Step [20/31], Loss: 0.41686\n",
      "Step [30/31], Loss: 0.40735\n",
      "Time elasped: 2.806098461151123\n",
      "-------TRAINING shd_rnn_1024_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50578\n",
      "Step [20/31], Loss: 0.47730\n",
      "Step [30/31], Loss: 0.46554\n",
      "Time elasped: 2.3981478214263916\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45037\n",
      "Step [20/31], Loss: 0.44308\n",
      "Step [30/31], Loss: 0.43905\n",
      "Time elasped: 2.3709776401519775\n",
      "-------TRAINING shd_rnn_1024_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49165\n",
      "Step [20/31], Loss: 0.46985\n",
      "Step [30/31], Loss: 0.45546\n",
      "Time elasped: 2.3992934226989746\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44113\n",
      "Step [20/31], Loss: 0.43183\n",
      "Step [30/31], Loss: 0.42394\n",
      "Time elasped: 2.4295871257781982\n",
      "-------TRAINING shd_rnn_1024_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51426\n",
      "Step [20/31], Loss: 0.46825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [30/31], Loss: 0.45270\n",
      "Time elasped: 3.0053884983062744\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43350\n",
      "Step [20/31], Loss: 0.42786\n",
      "Step [30/31], Loss: 0.41447\n",
      "Time elasped: 2.97042179107666\n",
      "-------TRAINING shd_rnn_1024_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52115\n",
      "Step [20/31], Loss: 0.47220\n",
      "Step [30/31], Loss: 0.45218\n",
      "Time elasped: 2.921910047531128\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43588\n",
      "Step [20/31], Loss: 0.42528\n",
      "Step [30/31], Loss: 0.41732\n",
      "Time elasped: 2.963710308074951\n",
      "-------TRAINING shd_rnn_1024_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50898\n",
      "Step [20/31], Loss: 0.48069\n",
      "Step [30/31], Loss: 0.46397\n",
      "Time elasped: 2.4685730934143066\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45480\n",
      "Step [20/31], Loss: 0.44409\n",
      "Step [30/31], Loss: 0.43652\n",
      "Time elasped: 2.4686245918273926\n",
      "-------TRAINING shd_rnn_1024_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50367\n",
      "Step [20/31], Loss: 0.47974\n",
      "Step [30/31], Loss: 0.46768\n",
      "Time elasped: 2.490130662918091\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45675\n",
      "Step [20/31], Loss: 0.44364\n",
      "Step [30/31], Loss: 0.43371\n",
      "Time elasped: 2.463484525680542\n",
      "-------TRAINING shd_rnn_1024_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50860\n",
      "Step [20/31], Loss: 0.47063\n",
      "Step [30/31], Loss: 0.45455\n",
      "Time elasped: 2.7975895404815674\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43991\n",
      "Step [20/31], Loss: 0.42796\n",
      "Step [30/31], Loss: 0.42071\n",
      "Time elasped: 2.7930572032928467\n",
      "-------TRAINING shd_rnn_1024_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.57004\n",
      "Step [20/31], Loss: 0.47604\n",
      "Step [30/31], Loss: 0.45879\n",
      "Time elasped: 2.7868857383728027\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44561\n",
      "Step [20/31], Loss: 0.43728\n",
      "Step [30/31], Loss: 0.42556\n",
      "Time elasped: 2.7881503105163574\n",
      "-------TRAINING shd_rnn_2048_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50574\n",
      "Step [20/31], Loss: 0.47474\n",
      "Step [30/31], Loss: 0.46003\n",
      "Time elasped: 3.5517125129699707\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44590\n",
      "Step [20/31], Loss: 0.43640\n",
      "Step [30/31], Loss: 0.42093\n",
      "Time elasped: 3.6308999061584473\n",
      "-------TRAINING shd_rnn_2048_noadp.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.51184\n",
      "Step [20/31], Loss: 0.48532\n",
      "Step [30/31], Loss: 0.47293\n",
      "Time elasped: 3.5842020511627197\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46365\n",
      "Step [20/31], Loss: 0.45505\n",
      "Step [30/31], Loss: 0.45294\n",
      "Time elasped: 3.676292896270752\n",
      "-------TRAINING shd_rnn_2048_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52657\n",
      "Step [20/31], Loss: 0.47726\n",
      "Step [30/31], Loss: 0.47052\n",
      "Time elasped: 4.54115629196167\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45395\n",
      "Step [20/31], Loss: 0.44415\n",
      "Step [30/31], Loss: 0.43777\n",
      "Time elasped: 4.632328987121582\n",
      "-------TRAINING shd_rnn_2048_noadp_delay.t7 ---------\n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50185\n",
      "Step [20/31], Loss: 0.46607\n",
      "Step [30/31], Loss: 0.45128\n",
      "Time elasped: 4.504727840423584\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43520\n",
      "Step [20/31], Loss: 0.42311\n",
      "Step [30/31], Loss: 0.40926\n",
      "Time elasped: 4.634532690048218\n",
      "-------TRAINING shd_rnn_2048_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.49955\n",
      "Step [20/31], Loss: 0.47823\n",
      "Step [30/31], Loss: 0.46425\n",
      "Time elasped: 3.12030029296875\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.45690\n",
      "Step [20/31], Loss: 0.44499\n",
      "Step [30/31], Loss: 0.43797\n",
      "Time elasped: 3.0998449325561523\n",
      "-------TRAINING shd_rnn_2048_noadp.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.50509\n",
      "Step [20/31], Loss: 0.48427\n",
      "Step [30/31], Loss: 0.47176\n",
      "Time elasped: 3.1059019565582275\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.46061\n",
      "Step [20/31], Loss: 0.45036\n",
      "Step [30/31], Loss: 0.44585\n",
      "Time elasped: 3.1458754539489746\n",
      "-------TRAINING shd_rnn_2048_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52227\n",
      "Step [20/31], Loss: 0.46696\n",
      "Step [30/31], Loss: 0.45519\n",
      "Time elasped: 3.973264217376709\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.44190\n",
      "Step [20/31], Loss: 0.43917\n",
      "Step [30/31], Loss: 0.42798\n",
      "Time elasped: 3.895519971847534\n",
      "-------TRAINING shd_rnn_2048_noadp_delay.t7 ---------\n",
      "tau_m_h \n",
      "Epoch [1/2]\n",
      "Step [10/31], Loss: 0.52334\n",
      "Step [20/31], Loss: 0.47227\n",
      "Step [30/31], Loss: 0.45449\n",
      "Time elasped: 3.9136962890625\n",
      "Epoch [2/2]\n",
      "Step [10/31], Loss: 0.43893\n",
      "Step [20/31], Loss: 0.42622\n",
      "Step [30/31], Loss: 0.41617\n",
      "Time elasped: 3.987060546875\n"
     ]
    }
   ],
   "source": [
    "for i in range(num):\n",
    "\n",
    "    if x[i]!='adp':\n",
    "        tau_m = float(x[i]) \n",
    "    else:\n",
    "        tau_m = x[i]\n",
    "    num_hidden = int(y[i])\n",
    "    delay_mode = z[i]\n",
    "    vreset = int(w[i])\n",
    "\n",
    "    #tau_m = 0.8305\n",
    "    #snn = RSNN_delay(d='shd', num_hidden=128, thresh=0.3, decay=0.3, batch_size=batch_size, win=50, device=device)\n",
    "    \n",
    "    if delay_mode == 'nodelay':\n",
    "        delay_name = ''\n",
    "        snn = RSNN('shd', num_hidden=num_hidden, thresh=0.3, tau_m=tau_m, vreset=vreset, batch_size=batch_size, win=time_window, device=device)\n",
    "    else:\n",
    "        delay_name = '_delay'\n",
    "        snn = RSNN_d('shd', num_hidden=num_hidden, thresh=0.3, tau_m=tau_m, vreset=vreset, batch_size=batch_size, win=time_window, device=device)\n",
    "\n",
    "    snn.to(device)\n",
    "    \n",
    "    if tau_m != 'adp':\n",
    "        tau_name = ''\n",
    "    else:\n",
    "        tau_name = '_adp'\n",
    "        \n",
    "    if vreset==0:\n",
    "        vreset_name = ''\n",
    "    else:\n",
    "        vreset_name = '_vreset'\n",
    "    # training configuration\n",
    "    modelname = 'shd_rnn_{}{}{}{}.t7'.format(snn.num_hidden, tau_name, delay_name, vreset_name)\n",
    "    \n",
    "    print(\"-------TRAINING {} ---------\".format(modelname))\n",
    "    num_samples = train_dataset.images.size()[0]\n",
    "\n",
    "    # super pythonic way to extract the parameters that will have 'normal' learning rate\n",
    "    base_params = [getattr(snn,name.split('.')[0]).weight for name, _ in snn.state_dict().items() if name[0]=='f']\n",
    "\n",
    "    # setting different learning rate for tau_m, if neeeded\n",
    "    if tau_m=='adp':\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': base_params},\n",
    "            {'params': snn.tau_m_h, 'lr': learning_rate * 10.0}],\n",
    "            lr=learning_rate)\n",
    "    else:    \n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': base_params}],\n",
    "            lr=learning_rate)\n",
    "\n",
    "    act_fun = ActFun.apply\n",
    "\n",
    "\n",
    "    # training loop\n",
    "    taus_m = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch [%d/%d]'  % (epoch + 1, num_epochs))\n",
    "        start_time = time.time()\n",
    "        snn.train_step(train_loader, optimizer=optimizer, criterion=nn.MSELoss(), num_samples = num_samples)\n",
    "        t = time.time() - start_time\n",
    "        print('Time elasped:', time.time() - start_time)\n",
    "\n",
    "        # update learning rate\n",
    "        optimizer = snn.lr_scheduler(optimizer, lr_decay_epoch=1)\n",
    "\n",
    "        # weight and decay recording\n",
    "        # taus_m.append((snn.tau_m_h.data.detach().clone(), snn.tau_m_o.data.detach().clone()))\n",
    "\n",
    "        if (epoch+1) % 5 ==0:\n",
    "            snn.test(test_loader, criterion=nn.MSELoss())\n",
    "            snn.save_model(modelname)   \n",
    "\n",
    "    with open('training_log', 'a') as logs:\n",
    "        logs.write(\"\\nFinished training {} epochs for {}, batch_size {}, time_per_epoch {} s\".format(num_epochs, modelname, batch_size, t))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "raising-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_model(modelname, 256, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
