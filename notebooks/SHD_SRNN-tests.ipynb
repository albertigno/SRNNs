{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alberto/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: albertigno\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from MyLargeDataset import *\n",
    "from snn_models import *\n",
    "\n",
    "import torch, time, os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "thresh = 0.3\n",
    "decay = 0.3\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "time_window = 50 # shd\n",
    "#time_window = 15 # shd\n",
    "dataset_path = r'./../../datasets'\n",
    "\n",
    "best_acc = 0\n",
    "acc_record = list([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n",
      "loading test set...\n",
      "torch.Size([2264, 50, 700])\n",
      "torch.Size([2264])\n",
      "num sample: 2264\n",
      "torch.Size([2264, 50, 700]) torch.Size([2264, 20])\n",
      "loading training set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed93fffce4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hd_digits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading training set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hd_digits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading data with pytorch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
      "\u001b[0;32m~/SRNNs/notebooks/MyLargeDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, method, lens, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spikes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_data_generator_from_hdf5_spikes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m#self.images = x.to_dense()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SRNNs/notebooks/MyLargeDataset.py\u001b[0m in \u001b[0;36msparse_data_generator_from_hdf5_spikes\u001b[0;34m(self, X, y, nb_steps, nb_units, max_time, shuffle)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mcoo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcoo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mcoo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "train_path = dataset_path+'/shd_digits/shd_train.h5'\n",
    "#train_path = dataset_path+'/shd_digits/shd_test.h5'\n",
    "test_path = dataset_path+'/shd_digits/shd_test.h5'\n",
    "# load datasets\n",
    "print(\"loading test set...\")\n",
    "test_dataset = MyDataset(test_path, 'hd_digits', time_window, device)\n",
    "print(\"loading training set...\")\n",
    "train_dataset = MyDataset(train_path, 'hd_digits', time_window, device)\n",
    "print(\"loading data with pytorch\")\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, drop_last=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = RSNN_delay(d='shd', num_hidden=64, thresh=0.3, decay=0.3, batch_size=batch_size, win=time_window, device=device)\n",
    "#snn = RSNN(d='shd', num_hidden=64, thresh=0.3, decay=0.3, batch_size=batch_size, win=time_window, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "    \n",
    "snn.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "modelname = 'shd_rnn_{}.t7'.format(snn.num_hidden)\n",
    "\n",
    "def lr_scheduler(optimizer, epoch, init_lr=0.1, lr_decay_epoch=100):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0 and epoch > 1:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.98\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(snn.parameters(), lr=learning_rate)\n",
    "\n",
    "act_fun = ActFun.apply\n",
    "print(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "\n",
    "    \n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    total_loss_train = 0\n",
    "    start_time = time.time()\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        snn.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = snn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        total_loss_train += loss.item()\n",
    "        total += labels.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            # print( outputs.sum(dim=0) )\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.5f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, running_loss))\n",
    "            running_loss = 0\n",
    "            print('Time elasped:', time.time() - start_time)\n",
    "    train_loss.append(total_loss_train / total)\n",
    "    \n",
    "    optimizer = lr_scheduler(optimizer, epoch, learning_rate, 1)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss_test = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = snn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, reference = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == reference).sum()\n",
    "        total_loss_test += loss.item() \n",
    "\n",
    "    print('Iters:', epoch, '\\n\\n\\n')\n",
    "    print('Test Accuracy of the model on the test samples: %.3f' % (100 * correct.float() / total))\n",
    "    \n",
    "    print('Train loss: {}, Test loss: {}'.format(total_loss_train, total_loss_test))\n",
    "    \n",
    "    acc = 100. * float(correct) / float(total)\n",
    "    acc_record.append(acc)\n",
    "    \n",
    "    test_loss.append(total_loss_test / total)\n",
    "    state = {\n",
    "    'net': snn.state_dict(),\n",
    "    'acc': acc,\n",
    "    'epoch': epoch,\n",
    "    'acc_record': acc_record,\n",
    "    'train_loss': train_loss,\n",
    "    'test_loss': test_loss\n",
    "    }    \n",
    "    \n",
    "    if acc>best_acc:\n",
    "        print('Saving best accuracy so far..')\n",
    "        print(acc)\n",
    "        torch.save(state, './checkpoint/' + modelname,  _use_new_zipfile_serialization=False)\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.d_ho.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "'net': snn.state_dict(),\n",
    "'acc': acc,\n",
    "    \n",
    "    \n",
    "    \n",
    "'epoch': epoch,\n",
    "'acc_record': acc_record,\n",
    "'train_loss': train_loss,\n",
    "'test_loss': test_loss\n",
    "}    \n",
    "\n",
    "mf = modelname.split('.')[0] + '_final.t7'\n",
    "torch.save(state, './checkpoint/' + mf ,  _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float().to(device)\n",
    "        outputs = model(images, win)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels).sum()\n",
    "    \n",
    "    acc = 100 * correct.float() / total\n",
    "    return acc\n",
    "\n",
    "acc = get_accuracy(snn)\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %.3f' % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"loading trained model\")\n",
    "model_to_load = modelname\n",
    "\n",
    "#snn_state_dict = torch.load('./checkpoint/'+modelname, map_location=torch.device('cpu'))['net']\n",
    "snn_state_dict = torch.load('./checkpoint/'+model_to_load, map_location=torch.device('cpu'))['net']\n",
    "print(snn_state_dict.keys())\n",
    "\n",
    "layers_location = 'checkpoint/'+model_to_load.split('.')[0]\n",
    "\n",
    "if not os.path.isdir(layers_location):\n",
    "    os.mkdir(layers_location)\n",
    "\n",
    "weights_biases = []\n",
    "for k in snn_state_dict:\n",
    "    np.savez(layers_location+'/'+k,snn_state_dict[k].data.cpu().numpy())\n",
    "    weights_biases.append(snn_state_dict[k].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = modelname\n",
    "#model_to_load = 'heidelberg_rnn_400_mem1.t7'\n",
    "snn_state_dict = torch.load('./checkpoint/'+model_to_load, map_location=torch.device('cpu'))['net']\n",
    "\n",
    "model = SNN_Model()\n",
    "model.load_state_dict(snn_state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# run a subset of test set\n",
    "num_to_test = 50\n",
    "for images, labels in test_loader:\n",
    "    images = images.float().to(device)\n",
    "    labels = labels.float().to(device)\n",
    "    outputs = model(images, win)\n",
    "    _, predicted = torch.max(outputs[:num_to_test,:].data, 1)\n",
    "    _, reference = torch.max(labels[:num_to_test,:].data, 1)\n",
    "    correct = (predicted == reference).sum()\n",
    "    break\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %.3f' % (100 * correct.float() / num_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpiNNaker: 50 muestras, 58%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
